{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "D - go through all the columns, and figure out what the null values are represented as\n",
    "\n",
    "D - remove constant columns again, after setting all the null values in the columns\n",
    "\n",
    "D - impute NaN with entire train and test data: https://www.kaggle.com/cbrogan/titanic/xgboost-example-python\n",
    "\n",
    "D - find all categorical columns (that are not binary), and one-hot encode them\n",
    "\n",
    "D - Go through this tutorial: XGB Guidelines\n",
    "    http://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n",
    "\n",
    "D - check the columns in the original data with 9999999999 - may need to modify for some sort of ratio - not necessarily NaN\n",
    "\n",
    "- check columns that contain the word \"saldo\" - features that are linear combinations of other features\n",
    "    https://www.kaggle.com/sionek/santander-customer-satisfaction/reverse-feature-engineering/log\n",
    "\n",
    "- Check for weird columns, possibly useless\n",
    "\n",
    "- get list of features and their importances, and throw out useless ones\n",
    "    https://www.kaggle.com/mmueller/liberty-mutual-group-property-inspection-prediction/xgb-feature-importance-python/code\n",
    "    http://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/\n",
    "\n",
    "find advanced ways to deal with training data that has unbalanced labels\n",
    "    - weighting - svm, logistic regression\n",
    "    - SMOTE\n",
    "        - http://comments.gmane.org/gmane.comp.python.scikit-learn/5278\n",
    "        - https://github.com/fmfn/UnbalancedDataset\n",
    "    - create L = # majority / # minority models, and ensemble them\n",
    "        - use different algorithms - i.e. svm and logistic regression with weights\n",
    "    - http://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/\n",
    "\n",
    "transform data that has exponentially growing values, i.e. log\n",
    "\n",
    "modify .9 predictions to 1.0, etc\n",
    "\n",
    "try blending and stacking http://mlwave.com/kaggle-ensembling-guide/\n",
    "    - logistic regression\n",
    "    - tensorflow\n",
    "\n",
    "\n",
    "CURRENT TODO:s\n",
    "D - fine-tune xgb models and submit to kaggle\n",
    "    D - fine-tune w 999999 set as null\n",
    "    - add one more step to processed data - impute var3\n",
    "- create tensorflow models\n",
    "- create logistic regression models\n",
    "    - combine feature selection from lasso and from tree-based\n",
    "- stacked generalization and blending models\n",
    "    - using xgb\n",
    "    - using lr\n",
    "    - using tensorflow\n",
    "- ensembling models\n",
    "    - using xgb, lr, and tensorflow predictions\n",
    "- unbalanced labels - create logistic regression models and svm models - create L = # majority / # minority models, and ensemble them\n",
    "- try between robust_scale and log transforming absolute values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import csv\n",
    "import datetime\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import sklearn\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(rc={\n",
    "       \"figure.figsize\": (16, 10),\n",
    "       \"axes.titlesize\": 14})\n",
    "\n",
    "from IPython.display import Image, display\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<style>.container { width:100% !important; }</style>\")\n",
    "\n",
    "from os.path import expanduser\n",
    "sys.path.insert(1, '{}/datsci'.format(expanduser('~')))\n",
    "from datsci import eda, munge\n",
    "from datsci import kaggle as kg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import santander\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNC\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.ensemble import RandomForestRegressor as RFR\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier as ABC\n",
    "from sklearn.ensemble import GradientBoostingClassifier as GBC\n",
    "from sklearn.linear_model import SGDClassifier as SGDClf\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stage</th>\n",
       "      <th>train rows</th>\n",
       "      <th>train cols</th>\n",
       "      <th>test rows</th>\n",
       "      <th>test cols</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>raw</td>\n",
       "      <td>76020</td>\n",
       "      <td>371</td>\n",
       "      <td>75818</td>\n",
       "      <td>369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dedup</td>\n",
       "      <td>71213</td>\n",
       "      <td>307</td>\n",
       "      <td>75818</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bin onehot</td>\n",
       "      <td>71213</td>\n",
       "      <td>363</td>\n",
       "      <td>75818</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>71213</td>\n",
       "      <td>357</td>\n",
       "      <td>75818</td>\n",
       "      <td>356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>impute mean</td>\n",
       "      <td>71213</td>\n",
       "      <td>357</td>\n",
       "      <td>75818</td>\n",
       "      <td>356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>impute median</td>\n",
       "      <td>71179</td>\n",
       "      <td>357</td>\n",
       "      <td>75818</td>\n",
       "      <td>356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>impute freq</td>\n",
       "      <td>71179</td>\n",
       "      <td>357</td>\n",
       "      <td>75818</td>\n",
       "      <td>356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>onehot int</td>\n",
       "      <td>71213</td>\n",
       "      <td>398</td>\n",
       "      <td>75818</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>rm test const</td>\n",
       "      <td>71213</td>\n",
       "      <td>390</td>\n",
       "      <td>75818</td>\n",
       "      <td>389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           stage  train rows  train cols  test rows  test cols\n",
       "0            raw       76020         371      75818        369\n",
       "1          dedup       71213         307      75818        306\n",
       "2     bin onehot       71213         363      75818        362\n",
       "3            NaN       71213         357      75818        356\n",
       "4    impute mean       71213         357      75818        356\n",
       "5  impute median       71179         357      75818        356\n",
       "6    impute freq       71179         357      75818        356\n",
       "7     onehot int       71213         398      75818        397\n",
       "8  rm test const       71213         390      75818        389"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO delete after comparing w new files - already implemented in santander.py\n",
    "def get_sizes(train_csv, test_csv):\n",
    "    df = pd.read_csv(train_csv)\n",
    "    df_test = pd.read_csv(test_csv, index_col='ID')\n",
    "    train_rows, train_cols = df.shape\n",
    "    test_rows, test_cols = df_test.shape\n",
    "    return train_rows, train_cols, test_rows, test_cols\n",
    "\n",
    "\n",
    "data_shapes = []\n",
    "for s, train_csv, test_csv in [\n",
    "    ('raw',           FILE_TRAIN,                                 FILE_TEST),\n",
    "    ('dedup',         FILE_TRAIN_DEDUP,                           FILE_TEST_DEDUP),\n",
    "    ('bin onehot',    FILE_TRAIN_DEDUP_ONEHOT,                    FILE_TEST_DEDUP_ONEHOT),\n",
    "    ('NaN',           FILE_TRAIN_DEDUP_ONEHOT_NA,                 FILE_TEST_DEDUP_ONEHOT_NA),\n",
    "    ('impute mean',   FILE_TRAIN_DEDUP_ONEHOT_NA_IMPUTE_MEAN,     FILE_TEST_DEDUP_ONEHOT_NA_IMPUTE_MEAN),\n",
    "    ('impute median', FILE_TRAIN_DEDUP_ONEHOT_NA_IMPUTE_MEDIAN,   FILE_TEST_DEDUP_ONEHOT_NA_IMPUTE_MEDIAN),\n",
    "    ('impute freq',   FILE_TRAIN_DEDUP_ONEHOT_NA_IMPUTE_FREQ,     FILE_TEST_DEDUP_ONEHOT_NA_IMPUTE_FREQ),\n",
    "    ('onehot int',    FILE_TRAIN_DEDUP_ONEHOT_NA_ONEHOTINT,       FILE_TEST_DEDUP_ONEHOT_NA_ONEHOTINT),\n",
    "    ('rm test const', FILE_TRAIN_DEDUP_ONEHOT_NA_ONEHOTINT_1TEST, FILE_TEST_DEDUP_ONEHOT_NA_ONEHOTINT_1TEST),]:\n",
    "    data_shapes.append((s,) + get_sizes(train_csv, test_csv))\n",
    "pd.DataFrame(data_shapes, columns=['stage', 'train rows', 'train cols', 'test rows', 'test cols'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cv_fit_model(model, X_train, y_train, X_test, y_test, cv_nfold=5, early_stopping_rounds=50, missing=None):\n",
    "    \n",
    "    # Train cv\n",
    "    xgb_param = model.get_xgb_params()\n",
    "    dtrain = xgb.DMatrix(X_train.values, label=y_train.values, missing=missing)\n",
    "    cv_result = xgb.cv(\n",
    "        xgb_param, dtrain, num_boost_round=model.get_params()['n_estimators'], nfold=cv_nfold,\n",
    "        metrics=['auc'], early_stopping_rounds=early_stopping_rounds, show_progress=False)\n",
    "    best_n_estimators = cv_result.shape[0]\n",
    "    model.set_params(n_estimators=best_n_estimators)\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train, y_train, eval_metric='auc')\n",
    "        \n",
    "    # Predict training data\n",
    "    y_hat_train = model.predict(X_train)\n",
    "\n",
    "    # Predict test data\n",
    "    y_hat_test = model.predict(X_test)\n",
    "    \n",
    "    # Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"best n_estimators: {}\".format(best_n_estimators))\n",
    "    print(\"AUC Score (Train): %f\" % roc_auc_score(y_train, y_hat_train))\n",
    "    print(\"AUC Score (Test) : %f\" % roc_auc_score(y_test,  y_hat_test))\n",
    "                    \n",
    "#     feat_imp = pd.Series(model.booster().get_fscore()).sort_values(ascending=False)\n",
    "#     feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "#     plt.ylabel('Feature Importance Score')\n",
    "\n",
    "model = xgb.XGBRegressor(\n",
    "    learning_rate =0.1,\n",
    "    n_estimators=1000,\n",
    "    objective= 'binary:logistic',\n",
    "    nthread=4,\n",
    "    seed=55,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Var3 null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until cv error hasn't decreased in 50 rounds.\n",
      "Stopping. Best iteration: 65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "best n_estimators: 66\n",
      "AUC Score (Train): 0.873196\n",
      "AUC Score (Test) : 0.835522\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test, feature_cols, df_train, df_test = santander.read_split(santander.FILE_TRAIN,\n",
    "                                                                                         santander.FILE_TEST)\n",
    "\n",
    "df_train, df_test = santander.set_var3_null(df_train, df_test)\n",
    "X_train, X_test = santander.set_var3_null(X_train, X_test)\n",
    "\n",
    "cv_fit_model(model, X_train, y_train, X_test, y_test, cv_nfold=5, early_stopping_rounds=50, missing=np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix 'delta' cols that contain 9999999999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df_train, df_test, feature_cols = santander.read_data(santander.FILE_TRAIN, santander.FILE_TEST)\n",
    "X_train, y_train, X_test, y_test, feature_cols, df_train, df_test = santander.read_split(santander.FILE_TRAIN,\n",
    "                                                                                         santander.FILE_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ratio_cols = []\n",
    "for c in df_train:\n",
    "    if 9999999999 in df_train[c].unique():\n",
    "        ratio_cols.append(c)\n",
    "        \n",
    "delta_cols = []\n",
    "for c in df_train:\n",
    "    if c.find('delta') == 0:\n",
    "        delta_cols.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, 26, True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ratio_cols), len(delta_cols), ratio_cols == delta_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['delta_imp_amort_var18_1y3',\n",
       " 'delta_imp_amort_var34_1y3',\n",
       " 'delta_imp_aport_var13_1y3',\n",
       " 'delta_imp_aport_var17_1y3',\n",
       " 'delta_imp_aport_var33_1y3',\n",
       " 'delta_imp_compra_var44_1y3',\n",
       " 'delta_imp_reemb_var13_1y3',\n",
       " 'delta_imp_reemb_var17_1y3',\n",
       " 'delta_imp_reemb_var33_1y3',\n",
       " 'delta_imp_trasp_var17_in_1y3',\n",
       " 'delta_imp_trasp_var17_out_1y3',\n",
       " 'delta_imp_trasp_var33_in_1y3',\n",
       " 'delta_imp_trasp_var33_out_1y3',\n",
       " 'delta_imp_venta_var44_1y3',\n",
       " 'delta_num_aport_var13_1y3',\n",
       " 'delta_num_aport_var17_1y3',\n",
       " 'delta_num_aport_var33_1y3',\n",
       " 'delta_num_compra_var44_1y3',\n",
       " 'delta_num_reemb_var13_1y3',\n",
       " 'delta_num_reemb_var17_1y3',\n",
       " 'delta_num_reemb_var33_1y3',\n",
       " 'delta_num_trasp_var17_in_1y3',\n",
       " 'delta_num_trasp_var17_out_1y3',\n",
       " 'delta_num_trasp_var33_in_1y3',\n",
       " 'delta_num_trasp_var33_out_1y3',\n",
       " 'delta_num_venta_var44_1y3']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratio_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0             76014\n",
       " 9999999999        4\n",
       "-1                 2\n",
       "Name: delta_num_trasp_var17_in_1y3, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 21\n",
    "c = ratio_cols[x]\n",
    "df_train[c].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0             75804\n",
       " 9999999999        8\n",
       "-1                 6\n",
       "Name: delta_num_trasp_var17_in_1y3, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[c].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    76016.000000\n",
       "mean        -0.000026\n",
       "std          0.005129\n",
       "min         -1.000000\n",
       "25%          0.000000\n",
       "50%          0.000000\n",
       "75%          0.000000\n",
       "max          0.000000\n",
       "Name: delta_num_trasp_var17_in_1y3, dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[df_train[c] != 9999999999][c].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    75810.000000\n",
       "mean        -0.000079\n",
       "std          0.008896\n",
       "min         -1.000000\n",
       "25%          0.000000\n",
       "50%          0.000000\n",
       "75%          0.000000\n",
       "max          0.000000\n",
       "Name: delta_num_trasp_var17_in_1y3, dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[df_test[c] != 9999999999][c].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((76020, 372), (75818, 370))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((76020, 373), (75818, 371))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "one-hot: 0, 1, 6, 9, 11, 12, 18\n",
    "modify: 2, 3, 4, 5, 13, 14, 15, 16, 17\n",
    "7, 8, 10, 19, 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(a - b) / b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a b c\n",
    "0 0 0\n",
    "0 1 1\n",
    "1 0 \n",
    "1 1 0"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "change 999999 to very negative numbers, i.e. -10\n",
    "change 999999 to very positive numbers, i.e. +10\n",
    "change 999999 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ratio_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until cv error hasn't decreased in 50 rounds.\n",
      "Stopping. Best iteration: 115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "best n_estimators: 116\n",
      "AUC Score (Train): 0.863213\n",
      "AUC Score (Test) : 0.833875\n"
     ]
    }
   ],
   "source": [
    "# Check performance of ratio columns w/o any modifications\n",
    "X_train, y_train, X_test, y_test, feature_cols, df_train, df_test = santander.read_split(santander.FILE_TRAIN,\n",
    "                                                                                         santander.FILE_TEST)\n",
    "\n",
    "cv_fit_model(model, X_train, y_train, X_test, y_test, cv_nfold=5, early_stopping_rounds=50, missing=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until cv error hasn't decreased in 50 rounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "best n_estimators: 116\n",
      "AUC Score (Train): 0.863213\n",
      "AUC Score (Test) : 0.833875\n"
     ]
    }
   ],
   "source": [
    "# change 999999 to very negative numbers, i.e. -10\n",
    "\n",
    "# df_train, df_test, feature_cols = santander.read_data(santander.FILE_TRAIN, santander.FILE_TEST)\n",
    "X_train, y_train, X_test, y_test, feature_cols, df_train, df_test = santander.read_split(santander.FILE_TRAIN,\n",
    "                                                                                        santander.FILE_TEST)\n",
    "\n",
    "replace_val = -10\n",
    "for c in ratio_cols:\n",
    "    if X_train[c].describe()['max'] == 9999999999:\n",
    "        X_train[c] = X_train[c].replace(9999999999, replace_val)\n",
    "    if X_test[c].describe()['max'] == 9999999999:\n",
    "        X_test[c] = X_test[c].replace(9999999999, replace_val)\n",
    "        \n",
    "cv_fit_model(model, X_train, y_train, X_test, y_test, cv_nfold=5, early_stopping_rounds=50, missing=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until cv error hasn't decreased in 50 rounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "best n_estimators: 116\n",
      "AUC Score (Train): 0.863213\n",
      "AUC Score (Test) : 0.833875\n"
     ]
    }
   ],
   "source": [
    "# change 999999 to very positive numbers, i.e. +10\n",
    "\n",
    "# df_train, df_test, feature_cols = santander.read_data(santander.FILE_TRAIN, santander.FILE_TEST)\n",
    "X_train, y_train, X_test, y_test, feature_cols, df_train, df_test = santander.read_split(santander.FILE_TRAIN,\n",
    "                                                                                        santander.FILE_TEST)\n",
    "\n",
    "\n",
    "replace_val = +10\n",
    "for c in ratio_cols:\n",
    "    if X_train[c].describe()['max'] == 9999999999:\n",
    "        X_train[c] = X_train[c].replace(9999999999, replace_val)\n",
    "    if X_test[c].describe()['max'] == 9999999999:\n",
    "        X_test[c] = X_test[c].replace(9999999999, replace_val)\n",
    "        \n",
    "cv_fit_model(model, X_train, y_train, X_test, y_test, cv_nfold=5, early_stopping_rounds=50, missing=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until cv error hasn't decreased in 50 rounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "best n_estimators: 116\n",
      "AUC Score (Train): 0.863213\n",
      "AUC Score (Test) : 0.833875\n"
     ]
    }
   ],
   "source": [
    "# change 999999 to 1\n",
    "\n",
    "# df_train, df_test, feature_cols = santander.read_data(santander.FILE_TRAIN, santander.FILE_TEST)\n",
    "X_train, y_train, X_test, y_test, feature_cols, df_train, df_test = santander.read_split(santander.FILE_TRAIN,\n",
    "                                                                                        santander.FILE_TEST)\n",
    "\n",
    "\n",
    "replace_val = 1\n",
    "for c in ratio_cols:\n",
    "    if X_train[c].describe()['max'] == 9999999999:\n",
    "        X_train[c] = X_train[c].replace(9999999999, replace_val)\n",
    "    if X_test[c].describe()['max'] == 9999999999:\n",
    "        X_test[c] = X_test[c].replace(9999999999, replace_val)\n",
    "        \n",
    "cv_fit_model(model, X_train, y_train, X_test, y_test, cv_nfold=5, early_stopping_rounds=50, missing=np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove duplicates and constant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(santander.FILE_TRAIN_DEDUP):\n",
    "    santander.read_process_write(santander.FILE_TRAIN,\n",
    "                                 santander.FILE_TEST,\n",
    "                                 santander.FILE_TRAIN_DEDUP,\n",
    "                                 santander.FILE_TEST_DEDUP,\n",
    "                                 santander.remove_duplicates_const)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encode binary features - cols that start with 'ind_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(santander.FILE_TRAIN_DEDUP_ONEHOT):\n",
    "    santander.read_process_write(santander.FILE_TRAIN_DEDUP,\n",
    "                                 santander.FILE_TEST_DEDUP,\n",
    "                                 santander.FILE_TRAIN_DEDUP_ONEHOT,\n",
    "                                 santander.FILE_TEST_DEDUP_ONEHOT,\n",
    "                                 santander.one_hot_encode_binary_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process known NaNs\n",
    "\n",
    "https://www.kaggle.com/c/santander-customer-satisfaction/forums/t/19291/data-dictionary/111360#post111360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def process_known_nans_bak():\n",
    "#     df = pd.read_csv(FILE_TRAIN_DEDUP_ONEHOT)\n",
    "#     feature_cols = list(df.columns)\n",
    "#     feature_cols.remove(TARGET_COL)\n",
    "#     df_test = pd.read_csv(FILE_TEST_DEDUP_ONEHOT, index_col='ID')\n",
    "    \n",
    "#     # Var3\n",
    "#     df['var3'] = df.var3.replace(-999999, np.nan)\n",
    "#     df_test['var3'] = df_test.var3.replace(-999999, np.nan)\n",
    "    \n",
    "#     # Find integer features with null values\n",
    "#     for c in feature_cols:\n",
    "#         if df[c].describe()['max'] == 9999999999:\n",
    "#             df[c] = df[c].replace(9999999999, np.nan)\n",
    "#             df_test[c] = df_test[c].replace(9999999999, np.nan)\n",
    "    \n",
    "#     # Remove constant columns\n",
    "#     df.drop(eda.find_const_cols(df), axis=1, inplace=True)\n",
    "\n",
    "#     # Remove duplicate columns and then rows again\n",
    "#     df = munge.remove_duplicates(df.T).T.drop_duplicates()\n",
    "    \n",
    "#     # Write to file\n",
    "#     df.to_csv(FILE_TRAIN_DEDUP_ONEHOT_NA, index=False)\n",
    "#     feature_cols = list(df.columns)\n",
    "#     feature_cols.remove(TARGET_COL)\n",
    "#     df_test[feature_cols].to_csv(FILE_TEST_DEDUP_ONEHOT_NA)\n",
    "    \n",
    "    \n",
    "# if not os.path.exists(FILE_TRAIN_DEDUP_ONEHOT_NA):\n",
    "#     process_known_nans()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill in null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(santander.FILE_TRAIN_DEDUP_ONEHOT_NA_IMPUTE_MEAN):\n",
    "    santander.read_process_write(santander.FILE_TRAIN_DEDUP_ONEHOT_NA,\n",
    "                                 santander.FILE_TEST_DEDUP_ONEHOT_NA,\n",
    "                                 santander.FILE_TRAIN_DEDUP_ONEHOT_NA_IMPUTE_MEAN,\n",
    "                                 santander.FILE_TEST_DEDUP_ONEHOT_NA_IMPUTE_MEAN,\n",
    "                                 santander.impute_null_vals,\n",
    "                                 pass_features=True,\n",
    "                                 process_kwargs={'strategy': 'mean'})\n",
    "    \n",
    "if not os.path.exists(santander.FILE_TRAIN_DEDUP_ONEHOT_NA_IMPUTE_MEDIAN):\n",
    "    santander.read_process_write(santander.FILE_TRAIN_DEDUP_ONEHOT_NA,\n",
    "                                 santander.FILE_TEST_DEDUP_ONEHOT_NA,\n",
    "                                 santander.FILE_TRAIN_DEDUP_ONEHOT_NA_IMPUTE_MEDIAN,\n",
    "                                 santander.FILE_TEST_DEDUP_ONEHOT_NA_IMPUTE_MEDIAN,\n",
    "                                 santander.impute_null_vals,\n",
    "                                 pass_features=True,\n",
    "                                 process_kwargs={'strategy': 'median'})\n",
    "    \n",
    "if not os.path.exists(santander.FILE_TRAIN_DEDUP_ONEHOT_NA_IMPUTE_FREQ):\n",
    "    santander.read_process_write(santander.FILE_TRAIN_DEDUP_ONEHOT_NA,\n",
    "                                 santander.FILE_TEST_DEDUP_ONEHOT_NA,\n",
    "                                 santander.FILE_TRAIN_DEDUP_ONEHOT_NA_IMPUTE_FREQ,\n",
    "                                 santander.FILE_TEST_DEDUP_ONEHOT_NA_IMPUTE_FREQ,\n",
    "                                 santander.impute_null_vals,\n",
    "                                 pass_features=True,\n",
    "                                 process_kwargs={'strategy': 'most_frequent'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn some of the integer columns to categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(santander.FILE_TRAIN_DEDUP_ONEHOT_NA_ONEHOTINT):\n",
    "    santander.read_process_write(santander.FILE_TRAIN_DEDUP_ONEHOT_NA,\n",
    "                                 santander.FILE_TEST_DEDUP_ONEHOT_NA,\n",
    "                                 santander.FILE_TRAIN_DEDUP_ONEHOT_NA_ONEHOTINT,\n",
    "                                 santander.FILE_TEST_DEDUP_ONEHOT_NA_ONEHOTINT,\n",
    "                                 santander.one_hot_int,\n",
    "                                 pass_features=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check step by step processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until cv error hasn't decreased in 50 rounds.\n",
      "/usr/local/lib/python3.5/site-packages/xgboost/training.py:272: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  idset = [randidx[(i * kstep): min(len(randidx), (i + 1) * kstep)] for i in range(nfold)]\n",
      "Stopping. Best iteration: 115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "best n_estimators: 116\n",
      "AUC Score (Train): 0.863213\n",
      "AUC Score (Test) : 0.833875\n"
     ]
    }
   ],
   "source": [
    "# Read data from file\n",
    "df_train, df_test, feature_cols = santander.read_data(santander.FILE_TRAIN, santander.FILE_TEST)\n",
    "\n",
    "# Split up the data\n",
    "X_all = df_train[feature_cols]  # feature values for all students\n",
    "y_all = df_train[santander.TARGET_COL]\n",
    "test_size = 0.3 # 30 percent\n",
    "X_train, X_test, y_train, y_test = santander.train_test_split(\n",
    "    X_all, y_all, test_size=test_size, random_state=0, stratify=y_all)\n",
    "\n",
    "cv_fit_model(model, X_train, y_train, X_test, y_test, cv_nfold=5, early_stopping_rounds=50, missing=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until cv error hasn't decreased in 50 rounds.\n",
      "/usr/local/lib/python3.5/site-packages/xgboost/training.py:272: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  idset = [randidx[(i * kstep): min(len(randidx), (i + 1) * kstep)] for i in range(nfold)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "best n_estimators: 116\n",
      "AUC Score (Train): 0.860659\n",
      "AUC Score (Test) : 0.841680\n"
     ]
    }
   ],
   "source": [
    "# Dedup and const\n",
    "df_train, df_test = santander.remove_duplicates_const(df_train, df_test)\n",
    "\n",
    "feature_cols = list(df_train.columns)\n",
    "feature_cols.remove(santander.TARGET_COL)\n",
    "\n",
    "# Split up the data\n",
    "X_all = df_train[feature_cols]  # feature values for all students\n",
    "y_all = df_train[santander.TARGET_COL]\n",
    "test_size = 0.3 # 30 percent\n",
    "X_train, X_test, y_train, y_test = santander.train_test_split(\n",
    "    X_all, y_all, test_size=test_size, random_state=0, stratify=y_all)\n",
    "\n",
    "cv_fit_model(model, X_train, y_train, X_test, y_test, cv_nfold=5, early_stopping_rounds=50, missing=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until cv error hasn't decreased in 50 rounds.\n",
      "/usr/local/lib/python3.5/site-packages/xgboost/training.py:272: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  idset = [randidx[(i * kstep): min(len(randidx), (i + 1) * kstep)] for i in range(nfold)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "best n_estimators: 116\n",
      "AUC Score (Train): 0.861306\n",
      "AUC Score (Test) : 0.842895\n"
     ]
    }
   ],
   "source": [
    "# Set var 3 null values\n",
    "df_train, df_test = santander.set_var3_null(df_train, df_test)\n",
    "\n",
    "feature_cols = list(df_train.columns)\n",
    "feature_cols.remove(santander.TARGET_COL)\n",
    "\n",
    "# Split up the data\n",
    "X_all = df_train[feature_cols]  # feature values for all students\n",
    "y_all = df_train[santander.TARGET_COL]\n",
    "test_size = 0.3 # 30 percent\n",
    "X_train, X_test, y_train, y_test = santander.train_test_split(\n",
    "    X_all, y_all, test_size=test_size, random_state=0, stratify=y_all)\n",
    "\n",
    "cv_fit_model(model, X_train, y_train, X_test, y_test, cv_nfold=5, early_stopping_rounds=50, missing=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until cv error hasn't decreased in 50 rounds.\n",
      "/usr/local/lib/python3.5/site-packages/xgboost/training.py:272: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  idset = [randidx[(i * kstep): min(len(randidx), (i + 1) * kstep)] for i in range(nfold)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "best n_estimators: 116\n",
      "AUC Score (Train): 0.861306\n",
      "AUC Score (Test) : 0.842895\n"
     ]
    }
   ],
   "source": [
    "# Fix 99999999 values in delta columns\n",
    "df_train, df_test = santander.fix_delta_cols(df_train, df_test, replace_with=1)\n",
    "\n",
    "feature_cols = list(df_train.columns)\n",
    "feature_cols.remove(santander.TARGET_COL)\n",
    "\n",
    "# Split up the data\n",
    "X_all = df_train[feature_cols]  # feature values for all students\n",
    "y_all = df_train[santander.TARGET_COL]\n",
    "test_size = 0.3 # 30 percent\n",
    "X_train, X_test, y_train, y_test = santander.train_test_split(\n",
    "    X_all, y_all, test_size=test_size, random_state=0, stratify=y_all)\n",
    "\n",
    "cv_fit_model(model, X_train, y_train, X_test, y_test, cv_nfold=5, early_stopping_rounds=50, missing=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until cv error hasn't decreased in 50 rounds.\n",
      "/usr/local/lib/python3.5/site-packages/xgboost/training.py:272: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  idset = [randidx[(i * kstep): min(len(randidx), (i + 1) * kstep)] for i in range(nfold)]\n",
      "Stopping. Best iteration: 107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "best n_estimators: 108\n",
      "AUC Score (Train): 0.860285\n",
      "AUC Score (Test) : 0.842551\n"
     ]
    }
   ],
   "source": [
    "# # Fix 99999999 values in delta columns\n",
    "\n",
    "# df_train, df_test, feature_cols = santander.read_data(santander.FILE_TRAIN, santander.FILE_TEST)\n",
    "# df_train, df_test = santander.remove_duplicates_const(df_train, df_test)\n",
    "# df_train, df_test = santander.set_var3_null(df_train, df_test)\n",
    "# df_train, df_test = santander.fix_delta_cols(df_train, df_test, replace_with=2)\n",
    "\n",
    "# feature_cols = list(df_train.columns)\n",
    "# feature_cols.remove(santander.TARGET_COL)\n",
    "\n",
    "# # Split up the data\n",
    "# X_all = df_train[feature_cols]  # feature values for all students\n",
    "# y_all = df_train[santander.TARGET_COL]\n",
    "# test_size = 0.3 # 30 percent\n",
    "# X_train, X_test, y_train, y_test = santander.train_test_split(\n",
    "#     X_all, y_all, test_size=test_size, random_state=0, stratify=y_all)\n",
    "\n",
    "# cv_fit_model(model, X_train, y_train, X_test, y_test, cv_nfold=5, early_stopping_rounds=50, missing=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until cv error hasn't decreased in 50 rounds.\n",
      "/usr/local/lib/python3.5/site-packages/xgboost/training.py:272: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  idset = [randidx[(i * kstep): min(len(randidx), (i + 1) * kstep)] for i in range(nfold)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "best n_estimators: 116\n",
      "AUC Score (Train): 0.861306\n",
      "AUC Score (Test) : 0.842895\n"
     ]
    }
   ],
   "source": [
    "# one hot encode binary\n",
    "df_train, df_test = santander.one_hot_encode_binary_features(df_train, df_test)\n",
    "\n",
    "feature_cols = list(df_train.columns)\n",
    "feature_cols.remove(santander.TARGET_COL)\n",
    "\n",
    "# Split up the data\n",
    "X_all = df_train[feature_cols]  # feature values for all students\n",
    "y_all = df_train[santander.TARGET_COL]\n",
    "test_size = 0.3 # 30 percent\n",
    "X_train, X_test, y_train, y_test = santander.train_test_split(\n",
    "    X_all, y_all, test_size=test_size, random_state=0, stratify=y_all)\n",
    "\n",
    "cv_fit_model(model, X_train, y_train, X_test, y_test, cv_nfold=5, early_stopping_rounds=50, missing=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save data to file\n",
    "santander.write_data(df_train,\n",
    "                     df_test, \n",
    "                     santander.FILE_TRAIN_DEDUP_VAR3_DELTA_1HOT,\n",
    "                     santander.FILE_TEST_DEDUP_VAR3_DELTA_1HOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data from file\n",
    "df_train, df_test, feature_cols = santander.read_data(santander.FILE_TRAIN_DEDUP_VAR3_DELTA1_1HOT,\n",
    "                                                      santander.FILE_TEST_DEDUP_VAR3_DELTA1_1HOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# One hot encode int\n",
    "df_train, df_test = santander.one_hot_int(df_train, df_test, feature_cols)\n",
    "\n",
    "feature_cols = list(df_train.columns)\n",
    "feature_cols.remove(santander.TARGET_COL)\n",
    "\n",
    "# Split up the data\n",
    "X_all = df_train[feature_cols]  # feature values for all students\n",
    "y_all = df_train[santander.TARGET_COL]\n",
    "test_size = 0.3 # 30 percent\n",
    "X_train, X_test, y_train, y_test = santander.train_test_split(\n",
    "    X_all, y_all, test_size=test_size, random_state=0, stratify=y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until cv error hasn't decreased in 100 rounds.\n",
      "/usr/local/lib/python3.5/site-packages/xgboost/training.py:272: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  idset = [randidx[(i * kstep): min(len(randidx), (i + 1) * kstep)] for i in range(nfold)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "best n_estimators: 116\n",
      "AUC Score (Train): 0.861654\n",
      "AUC Score (Test) : 0.842732\n"
     ]
    }
   ],
   "source": [
    "cv_fit_model(model, X_train, y_train, X_test, y_test, cv_nfold=5, early_stopping_rounds=50, missing=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save data to file\n",
    "santander.write_data(df_train,\n",
    "                     df_test, \n",
    "                     santander.FILE_TRAIN_DEDUP_VAR3_DELTA1_1HOT_1HOTINT,\n",
    "                     santander.FILE_TEST_DEDUP_VAR3_DELTA1_1HOT_1HOTINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    68398\n",
       "1.0     2815\n",
       "Name: TARGET, dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[santander.TARGET_COL].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11min 38s, sys: 32.6 s, total: 12min 11s\n",
      "Wall time: 12min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# With null values\n",
    "\n",
    "# Read from file\n",
    "df_train, df_test, feature_cols = santander.read_data(santander.FILE_TRAIN, santander.FILE_TEST)\n",
    "\n",
    "# Dedup and const\n",
    "df_train, df_test = santander.remove_duplicates_const(df_train, df_test)\n",
    "\n",
    "# Set var 3 null values\n",
    "df_train, df_test = santander.set_var3_null(df_train, df_test)\n",
    "\n",
    "# Fix 99999999 values in delta columns\n",
    "df_train, df_test = santander.fix_delta_cols(df_train, df_test, replace_with=np.nan)\n",
    "\n",
    "# One hot encode binary\n",
    "df_train, df_test = santander.one_hot_encode_binary_features(df_train, df_test)\n",
    "\n",
    "# Save data to file\n",
    "santander.write_data(df_train,\n",
    "                     df_test, \n",
    "                     santander.FILE_TRAIN_DEDUP_VAR3_DELTANAN_1HOT,\n",
    "                     santander.FILE_TEST_DEDUP_VAR3_DELTANAN_1HOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data from file\n",
    "df_train, df_test, feature_cols = santander.read_data(santander.FILE_TRAIN_DEDUP_VAR3_DELTANAN_1HOT,\n",
    "                                                      santander.FILE_TEST_DEDUP_VAR3_DELTANAN_1HOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# One hot encode int\n",
    "df_train, df_test = santander.one_hot_int(df_train, df_test, feature_cols, delta_nulltype=np.nan)\n",
    "\n",
    "feature_cols = list(df_train.columns)\n",
    "feature_cols.remove(santander.TARGET_COL)\n",
    "\n",
    "# Split up the data\n",
    "X_all = df_train[feature_cols]  # feature values for all students\n",
    "y_all = df_train[santander.TARGET_COL]\n",
    "test_size = 0.3 # 30 percent\n",
    "X_train, X_test, y_train, y_test = santander.train_test_split(\n",
    "    X_all, y_all, test_size=test_size, random_state=0, stratify=y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until cv error hasn't decreased in 50 rounds.\n",
      "/usr/local/lib/python3.5/site-packages/xgboost/training.py:272: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  idset = [randidx[(i * kstep): min(len(randidx), (i + 1) * kstep)] for i in range(nfold)]\n",
      "Stopping. Best iteration: 108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "best n_estimators: 109\n",
      "AUC Score (Train): 0.860122\n",
      "AUC Score (Test) : 0.842721\n"
     ]
    }
   ],
   "source": [
    "cv_fit_model(model, X_train, y_train, X_test, y_test, cv_nfold=5, early_stopping_rounds=50, missing=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save data to file\n",
    "santander.write_data(df_train,\n",
    "                     df_test, \n",
    "                     santander.FILE_TRAIN_DEDUP_VAR3_DELTANAN_1HOT_1HOTINT,\n",
    "                     santander.FILE_TEST_DEDUP_VAR3_DELTANAN_1HOT_1HOTINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "saldo_cols = []\n",
    "for c in df:\n",
    "    if c.find('saldo') > -1:\n",
    "        saldo_cols.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_var13_0\n",
      "num_var13_largo_0\n",
      "num_var13_largo\n",
      "num_var13_medio_0\n",
      "num_var13\n",
      "saldo_var13_corto\n",
      "saldo_var13_largo\n",
      "saldo_var13_medio\n",
      "saldo_var13\n",
      "delta_imp_aport_var13_1y3\n",
      "delta_num_aport_var13_1y3\n",
      "imp_aport_var13_hace3\n",
      "imp_aport_var13_ult1\n",
      "imp_reemb_var13_ult1\n",
      "num_aport_var13_hace3\n",
      "num_aport_var13_ult1\n",
      "saldo_medio_var13_corto_hace2\n",
      "saldo_medio_var13_corto_hace3\n",
      "saldo_medio_var13_corto_ult1\n",
      "saldo_medio_var13_corto_ult3\n",
      "saldo_medio_var13_largo_hace2\n",
      "saldo_medio_var13_largo_hace3\n",
      "saldo_medio_var13_largo_ult1\n",
      "saldo_medio_var13_largo_ult3\n",
      "saldo_medio_var13_medio_hace2\n",
      "saldo_medio_var13_medio_ult3\n",
      "onehot_ind_var13_0_0\n",
      "onehot_ind_var13_0_1\n",
      "onehot_ind_var13_corto_0_0\n",
      "onehot_ind_var13_corto_0_1\n",
      "onehot_ind_var13_corto_0\n",
      "onehot_ind_var13_corto_1\n",
      "onehot_ind_var13_largo_0_0\n",
      "onehot_ind_var13_largo_0_1\n",
      "onehot_ind_var13_largo_0\n",
      "onehot_ind_var13_largo_1\n",
      "onehot_ind_var13_medio_0_0\n",
      "onehot_ind_var13_medio_0_1\n",
      "onehot_ind_var13_0\n",
      "onehot_ind_var13_1\n",
      "onehot_num_reemb_var13_ult1_0\n",
      "onehot_num_reemb_var13_ult1_3\n",
      "onehot_num_var13_corto_0_3\n",
      "onehot_num_var13_corto_0_6\n",
      "onehot_num_var13_corto_3\n",
      "onehot_num_var13_corto_6\n",
      "onehot_num_meses_var13_corto_ult3_0\n",
      "onehot_num_meses_var13_corto_ult3_1\n",
      "onehot_num_meses_var13_corto_ult3_2\n",
      "onehot_num_meses_var13_corto_ult3_3\n",
      "onehot_num_meses_var13_largo_ult3_0\n",
      "onehot_num_meses_var13_largo_ult3_1\n",
      "onehot_num_meses_var13_largo_ult3_2\n",
      "onehot_num_meses_var13_largo_ult3_3\n"
     ]
    }
   ],
   "source": [
    "for c in df:\n",
    "    if c.find('var13') > -1:\n",
    "        print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['saldo_var1',\n",
       " 'saldo_var5',\n",
       " 'saldo_var6',\n",
       " 'saldo_var8',\n",
       " 'saldo_var12',\n",
       " 'saldo_var13_corto',\n",
       " 'saldo_var13_largo',\n",
       " 'saldo_var13_medio',\n",
       " 'saldo_var13',\n",
       " 'saldo_var14',\n",
       " 'saldo_var17',\n",
       " 'saldo_var18',\n",
       " 'saldo_var20',\n",
       " 'saldo_var24',\n",
       " 'saldo_var26',\n",
       " 'saldo_var25',\n",
       " 'saldo_var30',\n",
       " 'saldo_var31',\n",
       " 'saldo_var32',\n",
       " 'saldo_var33',\n",
       " 'saldo_var34',\n",
       " 'saldo_var37',\n",
       " 'saldo_var40',\n",
       " 'saldo_var42',\n",
       " 'saldo_var44',\n",
       " 'saldo_medio_var5_hace2',\n",
       " 'saldo_medio_var5_hace3',\n",
       " 'saldo_medio_var5_ult1',\n",
       " 'saldo_medio_var5_ult3',\n",
       " 'saldo_medio_var8_hace2',\n",
       " 'saldo_medio_var8_hace3',\n",
       " 'saldo_medio_var8_ult1',\n",
       " 'saldo_medio_var8_ult3',\n",
       " 'saldo_medio_var12_hace2',\n",
       " 'saldo_medio_var12_hace3',\n",
       " 'saldo_medio_var12_ult1',\n",
       " 'saldo_medio_var12_ult3',\n",
       " 'saldo_medio_var13_corto_hace2',\n",
       " 'saldo_medio_var13_corto_hace3',\n",
       " 'saldo_medio_var13_corto_ult1',\n",
       " 'saldo_medio_var13_corto_ult3',\n",
       " 'saldo_medio_var13_largo_hace2',\n",
       " 'saldo_medio_var13_largo_hace3',\n",
       " 'saldo_medio_var13_largo_ult1',\n",
       " 'saldo_medio_var13_largo_ult3',\n",
       " 'saldo_medio_var13_medio_hace2',\n",
       " 'saldo_medio_var13_medio_ult3',\n",
       " 'saldo_medio_var17_hace2',\n",
       " 'saldo_medio_var17_hace3',\n",
       " 'saldo_medio_var17_ult1',\n",
       " 'saldo_medio_var17_ult3',\n",
       " 'saldo_medio_var29_hace2',\n",
       " 'saldo_medio_var29_ult1',\n",
       " 'saldo_medio_var29_ult3',\n",
       " 'saldo_medio_var33_hace2',\n",
       " 'saldo_medio_var33_hace3',\n",
       " 'saldo_medio_var33_ult1',\n",
       " 'saldo_medio_var33_ult3',\n",
       " 'saldo_medio_var44_hace2',\n",
       " 'saldo_medio_var44_hace3',\n",
       " 'saldo_medio_var44_ult1',\n",
       " 'saldo_medio_var44_ult3']"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saldo_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal(df[['saldo_var13_corto', 'saldo_var13_largo', 'saldo_var13_medio']].sum(axis=1).values, df['saldo_var13'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>saldo_var13_corto</th>\n",
       "      <th>saldo_var13_largo</th>\n",
       "      <th>saldo_var13_medio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>300.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   saldo_var13_corto  saldo_var13_largo  saldo_var13_medio\n",
       "0                0.0                0.0                0.0\n",
       "1              300.0                0.0                0.0\n",
       "2                0.0                0.0                0.0\n",
       "3                0.0                0.0                0.0\n",
       "4                0.0                0.0                0.0"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['saldo_var13_corto', 'saldo_var13_largo', 'saldo_var13_medio']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
