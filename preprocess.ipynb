{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "D - go through all the columns, and figure out what the null values are represented as\n",
    "\n",
    "D - remove constant columns again, after setting all the null values in the columns\n",
    "\n",
    "D - impute NaN with entire train and test data: https://www.kaggle.com/cbrogan/titanic/xgboost-example-python\n",
    "\n",
    "D - find all categorical columns (that are not binary), and one-hot encode them\n",
    "\n",
    "D - find categorical columns that are off by one, i.e. 100000 in train while 99999 in test\n",
    "    - search for equal num unique, but not equal unique vals\n",
    "\n",
    "remove all columns that are constant in test file - see if the *other* value in the train data is heavily biased for another label\n",
    "\n",
    "check columns that contain the word \"saldo\"\n",
    "\n",
    "transform data that has exponentially growing values, i.e. log\n",
    "\n",
    "find advanced ways to deal with training data that has unbalanced labels - weighting?\n",
    "\n",
    "create more training data by slightly perturbing the sure data\n",
    "\n",
    "do stuff in kaggle blog - 2nd place\n",
    "\n",
    "modify .9 predictions to 1.0, etc\n",
    "\n",
    "try tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import csv\n",
    "import datetime\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import sklearn\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(rc={\n",
    "       \"figure.figsize\": (16, 10),\n",
    "       \"axes.titlesize\": 14})\n",
    "\n",
    "from IPython.display import Image, display\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<style>.container { width:100% !important; }</style>\")\n",
    "\n",
    "from os.path import expanduser\n",
    "sys.path.insert(1, '{}/datsci'.format(expanduser('~')))\n",
    "from datsci import eda, munge\n",
    "from datsci import kaggle as kg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FILE_TRAIN                                 = 'data/train.csv'\n",
    "FILE_TRAIN_DEDUP                           = 'data/train.dedup.csv'\n",
    "FILE_TRAIN_DEDUP_ONEHOT                    = 'data/train.dedup.onehot.csv'\n",
    "FILE_TRAIN_DEDUP_ONEHOT_NA                 = 'data/train.dedup.onehot.na.csv'\n",
    "FILE_TRAIN_DEDUP_ONEHOT_NA_IMPUTE_MEAN     = 'data/train.dedup.onehot.na.impute_mean.csv'\n",
    "FILE_TRAIN_DEDUP_ONEHOT_NA_IMPUTE_MEDIAN   = 'data/train.dedup.onehot.na.impute_median.csv'\n",
    "FILE_TRAIN_DEDUP_ONEHOT_NA_IMPUTE_FREQ     = 'data/train.dedup.onehot.na.impute_freq.csv'\n",
    "FILE_TRAIN_DEDUP_ONEHOT_NA_ONEHOTINT       = 'data/train.dedup.onehot.na.onehotint.csv'\n",
    "FILE_TRAIN_DEDUP_ONEHOT_NA_ONEHOTINT_1TEST = 'data/train.dedup.onehot.na.onehotint.1test.csv'\n",
    "\n",
    "FILE_TEST                                  = 'data/test.csv'\n",
    "FILE_TEST_DEDUP                            = 'data/test.dedup.csv'\n",
    "FILE_TEST_DEDUP_ONEHOT                     = 'data/test.dedup.onehot.csv'\n",
    "FILE_TEST_DEDUP_ONEHOT_NA                  = 'data/test.dedup.onehot.na.csv'\n",
    "FILE_TEST_DEDUP_ONEHOT_NA_IMPUTE_MEAN      = 'data/test.dedup.onehot.na.impute_mean.csv'\n",
    "FILE_TEST_DEDUP_ONEHOT_NA_IMPUTE_MEDIAN    = 'data/test.dedup.onehot.na.impute_median.csv'\n",
    "FILE_TEST_DEDUP_ONEHOT_NA_IMPUTE_FREQ      = 'data/test.dedup.onehot.na.impute_freq.csv'\n",
    "FILE_TEST_DEDUP_ONEHOT_NA_ONEHOTINT        = 'data/test.dedup.onehot.na.onehotint.csv'\n",
    "FILE_TEST_DEDUP_ONEHOT_NA_ONEHOTINT_1TEST  = 'data/test.dedup.onehot.na.onehotint.1test.csv'\n",
    "\n",
    "FILE_SAMPLE_SUBMIT                         = 'data/sample_submission.csv'\n",
    "\n",
    "TARGET_COL                                 = 'TARGET'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stage</th>\n",
       "      <th>train rows</th>\n",
       "      <th>train cols</th>\n",
       "      <th>test rows</th>\n",
       "      <th>test cols</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>raw</td>\n",
       "      <td>76020</td>\n",
       "      <td>371</td>\n",
       "      <td>75818</td>\n",
       "      <td>369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dedup</td>\n",
       "      <td>71213</td>\n",
       "      <td>307</td>\n",
       "      <td>75818</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bin onehot</td>\n",
       "      <td>71213</td>\n",
       "      <td>363</td>\n",
       "      <td>75818</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>71213</td>\n",
       "      <td>357</td>\n",
       "      <td>75818</td>\n",
       "      <td>356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>impute mean</td>\n",
       "      <td>71213</td>\n",
       "      <td>357</td>\n",
       "      <td>75818</td>\n",
       "      <td>356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>impute median</td>\n",
       "      <td>71179</td>\n",
       "      <td>357</td>\n",
       "      <td>75818</td>\n",
       "      <td>356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>impute freq</td>\n",
       "      <td>71179</td>\n",
       "      <td>357</td>\n",
       "      <td>75818</td>\n",
       "      <td>356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>onehot int</td>\n",
       "      <td>71213</td>\n",
       "      <td>398</td>\n",
       "      <td>75818</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>rm test const</td>\n",
       "      <td>71213</td>\n",
       "      <td>390</td>\n",
       "      <td>70895</td>\n",
       "      <td>389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           stage  train rows  train cols  test rows  test cols\n",
       "0            raw       76020         371      75818        369\n",
       "1          dedup       71213         307      75818        306\n",
       "2     bin onehot       71213         363      75818        362\n",
       "3            NaN       71213         357      75818        356\n",
       "4    impute mean       71213         357      75818        356\n",
       "5  impute median       71179         357      75818        356\n",
       "6    impute freq       71179         357      75818        356\n",
       "7     onehot int       71213         398      75818        397\n",
       "8  rm test const       71213         390      70895        389"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_sizes(train_csv, test_csv):\n",
    "    df = pd.read_csv(train_csv)\n",
    "    df_test = pd.read_csv(test_csv, index_col='ID')\n",
    "    train_rows, train_cols = df.shape\n",
    "    test_rows, test_cols = df_test.shape\n",
    "    return train_rows, train_cols, test_rows, test_cols\n",
    "\n",
    "\n",
    "data_shapes = []\n",
    "for s, train_csv, test_csv in [\n",
    "    ('raw',           FILE_TRAIN,                                 FILE_TEST),\n",
    "    ('dedup',         FILE_TRAIN_DEDUP,                           FILE_TEST_DEDUP),\n",
    "    ('bin onehot',    FILE_TRAIN_DEDUP_ONEHOT,                    FILE_TEST_DEDUP_ONEHOT),\n",
    "    ('NaN',           FILE_TRAIN_DEDUP_ONEHOT_NA,                 FILE_TEST_DEDUP_ONEHOT_NA),\n",
    "    ('impute mean',   FILE_TRAIN_DEDUP_ONEHOT_NA_IMPUTE_MEAN,     FILE_TEST_DEDUP_ONEHOT_NA_IMPUTE_MEAN),\n",
    "    ('impute median', FILE_TRAIN_DEDUP_ONEHOT_NA_IMPUTE_MEDIAN,   FILE_TEST_DEDUP_ONEHOT_NA_IMPUTE_MEDIAN),\n",
    "    ('impute freq',   FILE_TRAIN_DEDUP_ONEHOT_NA_IMPUTE_FREQ,     FILE_TEST_DEDUP_ONEHOT_NA_IMPUTE_FREQ),\n",
    "    ('onehot int',    FILE_TRAIN_DEDUP_ONEHOT_NA_ONEHOTINT,       FILE_TEST_DEDUP_ONEHOT_NA_ONEHOTINT),\n",
    "    ('rm test const', FILE_TRAIN_DEDUP_ONEHOT_NA_ONEHOTINT_1TEST, FILE_TEST_DEDUP_ONEHOT_NA_ONEHOTINT_1TEST),]:\n",
    "    data_shapes.append((s,) + get_sizes(train_csv, test_csv))\n",
    "pd.DataFrame(data_shapes, columns=['stage', 'train rows', 'train cols', 'test rows', 'test cols'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dedup():\n",
    "    # Read data from file\n",
    "    df = pd.read_csv(FILE_TRAIN, index_col='ID')\n",
    "\n",
    "    # Remove duplicate rows\n",
    "    df.drop_duplicates(inplace=True)\n",
    "\n",
    "    # Remove constant columns\n",
    "    df.drop(eda.find_const_cols(df), axis=1, inplace=True)\n",
    "    \n",
    "    # Remove duplicate columns and then rows again\n",
    "    df = munge.remove_duplicates(df.T).T.drop_duplicates()\n",
    "    \n",
    "    # Write to file\n",
    "    df.to_csv(FILE_TRAIN_DEDUP, index=False)\n",
    "    \n",
    "    feature_cols = list(df.columns)\n",
    "    feature_cols.remove(TARGET_COL)\n",
    "\n",
    "    # Read in test\n",
    "    df_test = pd.read_csv(FILE_TEST, index_col='ID')\n",
    "    df_test[feature_cols].to_csv(FILE_TEST_DEDUP)\n",
    "    \n",
    "    \n",
    "if not os.path.exists(FILE_TRAIN_DEDUP):\n",
    "    dedup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encode binary features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def one_hot_binary():\n",
    "    df = pd.read_csv(FILE_TRAIN_DEDUP)\n",
    "    feature_cols = list(df.columns)\n",
    "    feature_cols.remove(TARGET_COL)\n",
    "    df_test = pd.read_csv(FILE_TEST_DEDUP, index_col='ID')\n",
    "    \n",
    "    binary_cols = [c for c in df.columns if c[:4] == 'ind_']\n",
    "    \n",
    "    # Convert to int\n",
    "    for c in binary_cols:\n",
    "        df[c] = df[c].values.astype(int)\n",
    "    \n",
    "    df_onehot = munge.hash_features(df, columns=binary_cols)\n",
    "    df_test_onehot = munge.hash_features(df_test, columns=binary_cols)\n",
    "    \n",
    "    df_onehot.to_csv(FILE_TRAIN_DEDUP_ONEHOT, index=False)\n",
    "    df_test_onehot.to_csv(FILE_TEST_DEDUP_ONEHOT)\n",
    "\n",
    "\n",
    "if not os.path.exists(FILE_TRAIN_DEDUP_ONEHOT):\n",
    "    one_hot_binary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process known NaNs\n",
    "\n",
    "https://www.kaggle.com/c/santander-customer-satisfaction/forums/t/19291/data-dictionary/111360#post111360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_known_nans():\n",
    "    df = pd.read_csv(FILE_TRAIN_DEDUP_ONEHOT)\n",
    "    feature_cols = list(df.columns)\n",
    "    feature_cols.remove(TARGET_COL)\n",
    "    df_test = pd.read_csv(FILE_TEST_DEDUP_ONEHOT, index_col='ID')\n",
    "    \n",
    "    # Var3\n",
    "    df['var3'] = df.var3.replace(-999999, np.nan)\n",
    "    df_test['var3'] = df_test.var3.replace(-999999, np.nan)\n",
    "    \n",
    "    # Find integer features with null values\n",
    "    for c in feature_cols:\n",
    "        if df[c].describe()['max'] == 9999999999:\n",
    "            df[c] = df[c].replace(9999999999, np.nan)\n",
    "            df_test[c] = df_test[c].replace(9999999999, np.nan)\n",
    "    \n",
    "    # Remove constant columns\n",
    "    df.drop(eda.find_const_cols(df), axis=1, inplace=True)\n",
    "\n",
    "    # Remove duplicate columns and then rows again\n",
    "    df = munge.remove_duplicates(df.T).T.drop_duplicates()\n",
    "    \n",
    "    # Write to file\n",
    "    df.to_csv(FILE_TRAIN_DEDUP_ONEHOT_NA, index=False)\n",
    "    feature_cols = list(df.columns)\n",
    "    feature_cols.remove(TARGET_COL)\n",
    "    df_test[feature_cols].to_csv(FILE_TEST_DEDUP_ONEHOT_NA)\n",
    "    \n",
    "    \n",
    "if not os.path.exists(FILE_TRAIN_DEDUP_ONEHOT_NA):\n",
    "    process_known_nans()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill in null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "def impute_null_vals(train_csv, test_csv, train_out_csv, test_out_csv, strategy='mean'):\n",
    "\n",
    "    # Read in data\n",
    "    df = pd.read_csv(train_csv)\n",
    "    feature_cols = list(df.columns)\n",
    "    feature_cols.remove(TARGET_COL)\n",
    "    df_test = pd.read_csv(test_csv, index_col='ID')\n",
    "\n",
    "    # Impute using combined (train + test) datasets\n",
    "    df_combined = df[feature_cols].append(df_test[feature_cols])\n",
    "    imputer = Imputer(missing_values='NaN', strategy=strategy, axis=0, verbose=0, copy=False).fit(df_combined)\n",
    "    df[feature_cols] = imputer.transform(df[feature_cols])\n",
    "    df_test[feature_cols] = imputer.transform(df_test[feature_cols])\n",
    "    \n",
    "    # Remove duplicate columns and rows\n",
    "    df = munge.remove_duplicates(df.T).T.drop_duplicates()\n",
    "    feature_cols = list(df.columns)\n",
    "    feature_cols.remove(TARGET_COL)\n",
    "    df_test = df_test[feature_cols]\n",
    "    \n",
    "    # Write to file\n",
    "    df.to_csv(train_out_csv, index=False)\n",
    "    feature_cols = list(df.columns)\n",
    "    feature_cols.remove(TARGET_COL)\n",
    "    df_test[feature_cols].to_csv(test_out_csv)\n",
    "    \n",
    "    \n",
    "if not os.path.exists(FILE_TRAIN_DEDUP_ONEHOT_NA_IMPUTE_MEAN):\n",
    "    impute_null_vals(\n",
    "        FILE_TRAIN_DEDUP_ONEHOT_NA,               FILE_TEST_DEDUP_ONEHOT_NA,\n",
    "        FILE_TRAIN_DEDUP_ONEHOT_NA_IMPUTE_MEAN,   FILE_TEST_DEDUP_ONEHOT_NA_IMPUTE_MEAN,\n",
    "        strategy='mean'\n",
    "    )\n",
    "    \n",
    "if not os.path.exists(FILE_TRAIN_DEDUP_ONEHOT_NA_IMPUTE_MEDIAN):\n",
    "    impute_null_vals(\n",
    "        FILE_TRAIN_DEDUP_ONEHOT_NA,               FILE_TEST_DEDUP_ONEHOT_NA,\n",
    "        FILE_TRAIN_DEDUP_ONEHOT_NA_IMPUTE_MEDIAN, FILE_TEST_DEDUP_ONEHOT_NA_IMPUTE_MEDIAN,\n",
    "        strategy='median'\n",
    "    )\n",
    "    \n",
    "if not os.path.exists(FILE_TRAIN_DEDUP_ONEHOT_NA_IMPUTE_FREQ):\n",
    "    impute_null_vals(\n",
    "        FILE_TRAIN_DEDUP_ONEHOT_NA,               FILE_TEST_DEDUP_ONEHOT_NA,\n",
    "        FILE_TRAIN_DEDUP_ONEHOT_NA_IMPUTE_FREQ,   FILE_TEST_DEDUP_ONEHOT_NA_IMPUTE_FREQ,\n",
    "        strategy='most_frequent'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn some of the integer columns to categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_int():\n",
    "    df = pd.read_csv(FILE_TRAIN_DEDUP_ONEHOT_NA)\n",
    "    feature_cols = list(df.columns)\n",
    "    feature_cols.remove(TARGET_COL)\n",
    "    df_test = pd.read_csv(FILE_TEST_DEDUP_ONEHOT_NA, index_col='ID')\n",
    "    \n",
    "    # Ignore already-one hot encoded columns\n",
    "    int_cols = feature_cols[:]\n",
    "    for c in feature_cols:\n",
    "        if c[:6] == 'onehot':\n",
    "            int_cols.remove(c)\n",
    "           \n",
    "    # Fine categorical columns\n",
    "    categorical_cols = eda.find_categorical_columns(df[int_cols], df_test)\n",
    "    \n",
    "    # Convert non-null value containing columns to integers\n",
    "    for c, n in categorical_cols:\n",
    "        # Dont turn null values to int\n",
    "        if c not in {'delta_imp_trasp_var17_in_1y3', 'delta_imp_trasp_var33_in_1y3'}:\n",
    "            df[c] = df[c].values.astype(int)\n",
    "\n",
    "    # One-hot encode the categorical columns\n",
    "    catcols = list(map(lambda t: t[0], categorical_cols))\n",
    "    df_onehot = munge.hash_features(df, columns=catcols)\n",
    "    df_test_onehot = munge.hash_features(df_test, columns=catcols)\n",
    "\n",
    "    # Remove duplicate columns and rows\n",
    "    df_onehot = munge.remove_duplicates(df_onehot.T).T.drop_duplicates()\n",
    "    feature_cols = list(df_onehot.columns)\n",
    "    feature_cols.remove(TARGET_COL)\n",
    "    df_test_onehot = df_test_onehot[feature_cols]\n",
    "    \n",
    "    # Save to file\n",
    "    df_onehot.to_csv(FILE_TRAIN_DEDUP_ONEHOT_NA_ONEHOTINT, index=False)\n",
    "    df_test_onehot.to_csv(FILE_TEST_DEDUP_ONEHOT_NA_ONEHOTINT)\n",
    "\n",
    "    \n",
    "if not os.path.exists(FILE_TRAIN_DEDUP_ONEHOT_NA_ONEHOTINT):\n",
    "    one_hot_int()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find more categorical columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search for equal num unique, but not equal unique vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv(FILE_TRAIN_DEDUP_ONEHOT_NA_ONEHOTINT)\n",
    "# feature_cols = list(df.columns)\n",
    "# feature_cols.remove(TARGET_COL)\n",
    "# df_test = pd.read_csv(FILE_TEST_DEDUP_ONEHOT_NA_ONEHOTINT, index_col='ID')\n",
    "\n",
    "# int_cols = feature_cols[:]\n",
    "# for c in feature_cols:\n",
    "#     if c[:6] == 'onehot':\n",
    "#         int_cols.remove(c)\n",
    "        \n",
    "# same_counts = []\n",
    "# for c in int_cols:\n",
    "#     if df[c].nunique() - 1 == df_test[c].nunique():\n",
    "#         same_counts.append(c)\n",
    "        \n",
    "# x = 20\n",
    "# c = same_counts[x]\n",
    "# print(c)\n",
    "# df[c].value_counts()\n",
    "\n",
    "# df_test[c].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove columns that are constant in test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_test_const_cols():\n",
    "    df = pd.read_csv(FILE_TRAIN_DEDUP_ONEHOT_NA_ONEHOTINT)\n",
    "    feature_cols = list(df.columns)\n",
    "    feature_cols.remove(TARGET_COL)\n",
    "    df_test = pd.read_csv(FILE_TEST_DEDUP_ONEHOT_NA_ONEHOTINT, index_col='ID')\n",
    "    \n",
    "    # Find const cols in test file\n",
    "    test_const_cols = eda.find_const_cols(df_test)\n",
    "    \n",
    "    # Remove const cols\n",
    "    df.drop(test_const_cols, axis=1, inplace=True)\n",
    "    df_test.drop(test_const_cols, axis=1, inplace=True)\n",
    "    \n",
    "    # Remove duplicate rows\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df_test.drop_duplicates(inplace=True)\n",
    "    \n",
    "    # Save to file\n",
    "    df.to_csv(FILE_TRAIN_DEDUP_ONEHOT_NA_ONEHOTINT_1TEST, index=False)\n",
    "    df_test.to_csv(FILE_TEST_DEDUP_ONEHOT_NA_ONEHOTINT_1TEST)\n",
    "    \n",
    "if not os.path.exists(FILE_TRAIN_DEDUP_ONEHOT_NA_ONEHOTINT_1TEST):\n",
    "    remove_test_const_cols()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
