{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "D - go through all the columns, and figure out what the null values are represented as\n",
    "\n",
    "D - remove constant columns again, after setting all the null values in the columns\n",
    "\n",
    "D - impute NaN with entire train and test data: https://www.kaggle.com/cbrogan/titanic/xgboost-example-python\n",
    "\n",
    "D - find all categorical columns (that are not binary), and one-hot encode them\n",
    "\n",
    "D - find categorical columns that are off by one, i.e. 100000 in train while 99999 in test\n",
    "    - search for equal num unique, but not equal unique vals\n",
    "\n",
    "D - remove all columns that are constant in test file - see if the *other* value in the train data is heavily biased for another label\n",
    "\n",
    "Go through this tutorial: XGB Guidelines\n",
    "http://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n",
    "\n",
    "- check the columns in the original data with 9999999999 - may need to modify for some sort of ratio - not necessarily NaN\n",
    "\n",
    "- get list of features and their importances, and throw out useless ones\n",
    "    https://www.kaggle.com/mmueller/liberty-mutual-group-property-inspection-prediction/xgb-feature-importance-python/code\n",
    "    http://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/\n",
    "\n",
    "check columns that contain the word \"saldo\" - features that are linear combinations of other features\n",
    "    https://www.kaggle.com/sionek/santander-customer-satisfaction/reverse-feature-engineering/log\n",
    "\n",
    "- Check for weird columns, possibly useless\n",
    "\n",
    "find advanced ways to deal with training data that has unbalanced labels - weighting?\n",
    "    - create more training data by slightly perturbing the sure data\n",
    "    - create L = # majority / # minority models, and ensemble them\n",
    "        - use different algorithms - i.e. svm and logistic regression with weights\n",
    "    - http://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/\n",
    "\n",
    "transform data that has exponentially growing values, i.e. log\n",
    "\n",
    "try blending and stacking http://mlwave.com/kaggle-ensembling-guide/\n",
    "    - logistic regression\n",
    "    - tensorflow\n",
    "\n",
    "modify .9 predictions to 1.0, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import csv\n",
    "import datetime\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import sklearn\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(rc={\n",
    "       \"figure.figsize\": (16, 10),\n",
    "       \"axes.titlesize\": 14})\n",
    "\n",
    "from IPython.display import Image, display\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<style>.container { width:100% !important; }</style>\")\n",
    "\n",
    "from os.path import expanduser\n",
    "sys.path.insert(1, '{}/datsci'.format(expanduser('~')))\n",
    "from datsci import eda, munge\n",
    "from datsci import kaggle as kg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import santander\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNC\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.ensemble import RandomForestRegressor as RFR\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier as ABC\n",
    "from sklearn.ensemble import GradientBoostingClassifier as GBC\n",
    "from sklearn.linear_model import SGDClassifier as SGDClf\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FILE_TRAIN                                 = 'data/train.csv'\n",
    "FILE_TRAIN_DEDUP                           = 'data/train.dedup.csv'\n",
    "FILE_TRAIN_DEDUP_ONEHOT                    = 'data/train.dedup.onehot.csv'\n",
    "FILE_TRAIN_DEDUP_ONEHOT_NA                 = 'data/train.dedup.onehot.na.csv'\n",
    "FILE_TRAIN_DEDUP_ONEHOT_NA_IMPUTE_MEAN     = 'data/train.dedup.onehot.na.impute_mean.csv'\n",
    "FILE_TRAIN_DEDUP_ONEHOT_NA_IMPUTE_MEDIAN   = 'data/train.dedup.onehot.na.impute_median.csv'\n",
    "FILE_TRAIN_DEDUP_ONEHOT_NA_IMPUTE_FREQ     = 'data/train.dedup.onehot.na.impute_freq.csv'\n",
    "FILE_TRAIN_DEDUP_ONEHOT_NA_ONEHOTINT       = 'data/train.dedup.onehot.na.onehotint.csv'\n",
    "FILE_TRAIN_DEDUP_ONEHOT_NA_ONEHOTINT_1TEST = 'data/train.dedup.onehot.na.onehotint.1test.csv'\n",
    "\n",
    "FILE_TEST                                  = 'data/test.csv'\n",
    "FILE_TEST_DEDUP                            = 'data/test.dedup.csv'\n",
    "FILE_TEST_DEDUP_ONEHOT                     = 'data/test.dedup.onehot.csv'\n",
    "FILE_TEST_DEDUP_ONEHOT_NA                  = 'data/test.dedup.onehot.na.csv'\n",
    "FILE_TEST_DEDUP_ONEHOT_NA_IMPUTE_MEAN      = 'data/test.dedup.onehot.na.impute_mean.csv'\n",
    "FILE_TEST_DEDUP_ONEHOT_NA_IMPUTE_MEDIAN    = 'data/test.dedup.onehot.na.impute_median.csv'\n",
    "FILE_TEST_DEDUP_ONEHOT_NA_IMPUTE_FREQ      = 'data/test.dedup.onehot.na.impute_freq.csv'\n",
    "FILE_TEST_DEDUP_ONEHOT_NA_ONEHOTINT        = 'data/test.dedup.onehot.na.onehotint.csv'\n",
    "FILE_TEST_DEDUP_ONEHOT_NA_ONEHOTINT_1TEST  = 'data/test.dedup.onehot.na.onehotint.1test.csv'\n",
    "\n",
    "FILE_SAMPLE_SUBMIT                         = 'data/sample_submission.csv'\n",
    "\n",
    "TARGET_COL                                 = 'TARGET'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stage</th>\n",
       "      <th>train rows</th>\n",
       "      <th>train cols</th>\n",
       "      <th>test rows</th>\n",
       "      <th>test cols</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>raw</td>\n",
       "      <td>76020</td>\n",
       "      <td>371</td>\n",
       "      <td>75818</td>\n",
       "      <td>369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dedup</td>\n",
       "      <td>71213</td>\n",
       "      <td>307</td>\n",
       "      <td>75818</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bin onehot</td>\n",
       "      <td>71213</td>\n",
       "      <td>363</td>\n",
       "      <td>75818</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>71213</td>\n",
       "      <td>357</td>\n",
       "      <td>75818</td>\n",
       "      <td>356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>impute mean</td>\n",
       "      <td>71213</td>\n",
       "      <td>357</td>\n",
       "      <td>75818</td>\n",
       "      <td>356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>impute median</td>\n",
       "      <td>71179</td>\n",
       "      <td>357</td>\n",
       "      <td>75818</td>\n",
       "      <td>356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>impute freq</td>\n",
       "      <td>71179</td>\n",
       "      <td>357</td>\n",
       "      <td>75818</td>\n",
       "      <td>356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>onehot int</td>\n",
       "      <td>71213</td>\n",
       "      <td>398</td>\n",
       "      <td>75818</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>rm test const</td>\n",
       "      <td>71213</td>\n",
       "      <td>390</td>\n",
       "      <td>75818</td>\n",
       "      <td>389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           stage  train rows  train cols  test rows  test cols\n",
       "0            raw       76020         371      75818        369\n",
       "1          dedup       71213         307      75818        306\n",
       "2     bin onehot       71213         363      75818        362\n",
       "3            NaN       71213         357      75818        356\n",
       "4    impute mean       71213         357      75818        356\n",
       "5  impute median       71179         357      75818        356\n",
       "6    impute freq       71179         357      75818        356\n",
       "7     onehot int       71213         398      75818        397\n",
       "8  rm test const       71213         390      75818        389"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_sizes(train_csv, test_csv):\n",
    "    df = pd.read_csv(train_csv)\n",
    "    df_test = pd.read_csv(test_csv, index_col='ID')\n",
    "    train_rows, train_cols = df.shape\n",
    "    test_rows, test_cols = df_test.shape\n",
    "    return train_rows, train_cols, test_rows, test_cols\n",
    "\n",
    "\n",
    "data_shapes = []\n",
    "for s, train_csv, test_csv in [\n",
    "    ('raw',           FILE_TRAIN,                                 FILE_TEST),\n",
    "    ('dedup',         FILE_TRAIN_DEDUP,                           FILE_TEST_DEDUP),\n",
    "    ('bin onehot',    FILE_TRAIN_DEDUP_ONEHOT,                    FILE_TEST_DEDUP_ONEHOT),\n",
    "    ('NaN',           FILE_TRAIN_DEDUP_ONEHOT_NA,                 FILE_TEST_DEDUP_ONEHOT_NA),\n",
    "    ('impute mean',   FILE_TRAIN_DEDUP_ONEHOT_NA_IMPUTE_MEAN,     FILE_TEST_DEDUP_ONEHOT_NA_IMPUTE_MEAN),\n",
    "    ('impute median', FILE_TRAIN_DEDUP_ONEHOT_NA_IMPUTE_MEDIAN,   FILE_TEST_DEDUP_ONEHOT_NA_IMPUTE_MEDIAN),\n",
    "    ('impute freq',   FILE_TRAIN_DEDUP_ONEHOT_NA_IMPUTE_FREQ,     FILE_TEST_DEDUP_ONEHOT_NA_IMPUTE_FREQ),\n",
    "    ('onehot int',    FILE_TRAIN_DEDUP_ONEHOT_NA_ONEHOTINT,       FILE_TEST_DEDUP_ONEHOT_NA_ONEHOTINT),\n",
    "    ('rm test const', FILE_TRAIN_DEDUP_ONEHOT_NA_ONEHOTINT_1TEST, FILE_TEST_DEDUP_ONEHOT_NA_ONEHOTINT_1TEST),]:\n",
    "    data_shapes.append((s,) + get_sizes(train_csv, test_csv))\n",
    "pd.DataFrame(data_shapes, columns=['stage', 'train rows', 'train cols', 'test rows', 'test cols'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix 'delta' cols that contain 9999999999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train, df_test, feature_cols = santander.read_data(FILE_TRAIN, FILE_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ratio_cols = []\n",
    "for c in df_train:\n",
    "    if 9999999999 in df[c].unique():\n",
    "        ratio_cols.append(c)\n",
    "        \n",
    "delta_cols = []\n",
    "for c in df_train:\n",
    "    if c.find('delta') == 0:\n",
    "        delta_cols.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, 26, True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ratio_cols), len(delta_cols), ratio_cols == delta_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['delta_imp_amort_var18_1y3',\n",
       " 'delta_imp_amort_var34_1y3',\n",
       " 'delta_imp_aport_var13_1y3',\n",
       " 'delta_imp_aport_var17_1y3',\n",
       " 'delta_imp_aport_var33_1y3',\n",
       " 'delta_imp_compra_var44_1y3',\n",
       " 'delta_imp_reemb_var13_1y3',\n",
       " 'delta_imp_reemb_var17_1y3',\n",
       " 'delta_imp_reemb_var33_1y3',\n",
       " 'delta_imp_trasp_var17_in_1y3',\n",
       " 'delta_imp_trasp_var17_out_1y3',\n",
       " 'delta_imp_trasp_var33_in_1y3',\n",
       " 'delta_imp_trasp_var33_out_1y3',\n",
       " 'delta_imp_venta_var44_1y3',\n",
       " 'delta_num_aport_var13_1y3',\n",
       " 'delta_num_aport_var17_1y3',\n",
       " 'delta_num_aport_var33_1y3',\n",
       " 'delta_num_compra_var44_1y3',\n",
       " 'delta_num_reemb_var13_1y3',\n",
       " 'delta_num_reemb_var17_1y3',\n",
       " 'delta_num_reemb_var33_1y3',\n",
       " 'delta_num_trasp_var17_in_1y3',\n",
       " 'delta_num_trasp_var17_out_1y3',\n",
       " 'delta_num_trasp_var33_in_1y3',\n",
       " 'delta_num_trasp_var33_out_1y3',\n",
       " 'delta_num_venta_var44_1y3']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratio_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0.000000e+00    75923\n",
       " 1.000000e+10       70\n",
       "-1.000000e+00       18\n",
       " 1.000000e+00        3\n",
       " 4.000000e+00        2\n",
       "-3.333333e-01        1\n",
       "-6.666667e-01        1\n",
       " 2.500000e+00        1\n",
       " 5.000000e-01        1\n",
       "Name: delta_num_compra_var44_1y3, dtype: int64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 17\n",
    "c = ratio_cols[x]\n",
    "df[c].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0.000000e+00    75716\n",
       " 1.000000e+10       78\n",
       "-1.000000e+00       14\n",
       " 1.000000e+00        3\n",
       "-8.333333e-01        1\n",
       "-6.666667e-01        1\n",
       " 2.000000e-01        1\n",
       "-8.666667e-01        1\n",
       " 5.000000e+00        1\n",
       "-5.000000e-01        1\n",
       "-7.500000e-01        1\n",
       "Name: delta_num_compra_var44_1y3, dtype: int64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[c].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    75650.000000\n",
       "mean        -0.022113\n",
       "std          0.147300\n",
       "min         -1.000000\n",
       "25%          0.000000\n",
       "50%          0.000000\n",
       "75%          0.000000\n",
       "max          1.000000\n",
       "Name: delta_num_aport_var13_1y3, dtype: float64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[c] != 9999999999][c].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    75441.000000\n",
       "mean        -0.021805\n",
       "std          0.147538\n",
       "min         -1.000000\n",
       "25%          0.000000\n",
       "50%          0.000000\n",
       "75%          0.000000\n",
       "max          3.000000\n",
       "Name: delta_num_aport_var13_1y3, dtype: float64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[df_test[c] != 9999999999][c].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((76020, 372), (75818, 370))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = munge.hash_features(df, columns=[c])\n",
    "df_test = munge.hash_features(df_test, columns=[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((76020, 373), (75818, 371))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "one-hot: 0, 1, 6, 9, 11, 12\n",
    "modify: 2, 3, 4, 5, 13, 14, 15, 16, 17\n",
    "7, 8, 10, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(a - b) / b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a b c\n",
    "0 0 0\n",
    "0 1 1\n",
    "1 0 \n",
    "1 1 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "change 999999 to very negative numbers, i.e. -10\n",
    "change 999999 to very positive numbers, i.e. +10\n",
    "change 999999 to 1\n",
    "just add 3 columns increased, decreased, and stayed the same, and then change 999999 to null\n",
    "add 3 columns and remove orig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dedup_bak():\n",
    "    # Read data from file\n",
    "    df = pd.read_csv(FILE_TRAIN, index_col='ID')\n",
    "\n",
    "    # Remove duplicate rows\n",
    "    df.drop_duplicates(inplace=True)\n",
    "\n",
    "    # Remove constant columns\n",
    "    df.drop(eda.find_const_cols(df), axis=1, inplace=True)\n",
    "    \n",
    "    # Remove duplicate columns and then rows again\n",
    "    df = munge.remove_duplicates(df.T).T.drop_duplicates()\n",
    "    \n",
    "    # Write to file\n",
    "    df.to_csv(FILE_TRAIN_DEDUP, index=False)\n",
    "    \n",
    "    feature_cols = list(df.columns)\n",
    "    feature_cols.remove(TARGET_COL)\n",
    "\n",
    "    # Read in test\n",
    "    df_test = pd.read_csv(FILE_TEST, index_col='ID')\n",
    "    df_test[feature_cols].to_csv(FILE_TEST_DEDUP)\n",
    "        \n",
    "if not os.path.exists(FILE_TRAIN_DEDUP):\n",
    "    santander.csv_remove_duplicates_const(FILE_TRAIN, FILE_TEST,\n",
    "                                          FILE_TRAIN_DEDUP, FILE_TEST_DEDUP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encode binary features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def one_hot_binary_bak():\n",
    "    df = pd.read_csv(FILE_TRAIN_DEDUP)\n",
    "    feature_cols = list(df.columns)\n",
    "    feature_cols.remove(TARGET_COL)\n",
    "    df_test = pd.read_csv(FILE_TEST_DEDUP, index_col='ID')\n",
    "    \n",
    "    binary_cols = [c for c in df.columns if c[:4] == 'ind_']\n",
    "    \n",
    "    # Convert to int\n",
    "    for c in binary_cols:\n",
    "        df[c] = df[c].values.astype(int)\n",
    "    \n",
    "    df_onehot = munge.hash_features(df, columns=binary_cols)\n",
    "    df_test_onehot = munge.hash_features(df_test, columns=binary_cols)\n",
    "    \n",
    "    df_onehot.to_csv(FILE_TRAIN_DEDUP_ONEHOT, index=False)\n",
    "    df_test_onehot.to_csv(FILE_TEST_DEDUP_ONEHOT)\n",
    "\n",
    "\n",
    "if not os.path.exists(FILE_TRAIN_DEDUP_ONEHOT):\n",
    "    santander.csv_one_hot_encode_binary_features(FILE_TRAIN_DEDUP, FILE_TEST_DEDUP,\n",
    "                                                 FILE_TRAIN_DEDUP_ONEHOT, FILE_TEST_DEDUP_ONEHOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process known NaNs\n",
    "\n",
    "https://www.kaggle.com/c/santander-customer-satisfaction/forums/t/19291/data-dictionary/111360#post111360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_known_nans():\n",
    "    df = pd.read_csv(FILE_TRAIN_DEDUP_ONEHOT)\n",
    "    feature_cols = list(df.columns)\n",
    "    feature_cols.remove(TARGET_COL)\n",
    "    df_test = pd.read_csv(FILE_TEST_DEDUP_ONEHOT, index_col='ID')\n",
    "    \n",
    "    # Var3\n",
    "    df['var3'] = df.var3.replace(-999999, np.nan)\n",
    "    df_test['var3'] = df_test.var3.replace(-999999, np.nan)\n",
    "    \n",
    "    # Find integer features with null values\n",
    "    for c in feature_cols:\n",
    "        if df[c].describe()['max'] == 9999999999:\n",
    "            df[c] = df[c].replace(9999999999, np.nan)\n",
    "            df_test[c] = df_test[c].replace(9999999999, np.nan)\n",
    "    \n",
    "    # Remove constant columns\n",
    "    df.drop(eda.find_const_cols(df), axis=1, inplace=True)\n",
    "\n",
    "    # Remove duplicate columns and then rows again\n",
    "    df = munge.remove_duplicates(df.T).T.drop_duplicates()\n",
    "    \n",
    "    # Write to file\n",
    "    df.to_csv(FILE_TRAIN_DEDUP_ONEHOT_NA, index=False)\n",
    "    feature_cols = list(df.columns)\n",
    "    feature_cols.remove(TARGET_COL)\n",
    "    df_test[feature_cols].to_csv(FILE_TEST_DEDUP_ONEHOT_NA)\n",
    "    \n",
    "    \n",
    "if not os.path.exists(FILE_TRAIN_DEDUP_ONEHOT_NA):\n",
    "    process_known_nans()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill in null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "def impute_null_vals(train_csv, test_csv, train_out_csv, test_out_csv, strategy='mean'):\n",
    "\n",
    "    # Read in data\n",
    "    df = pd.read_csv(train_csv)\n",
    "    feature_cols = list(df.columns)\n",
    "    feature_cols.remove(TARGET_COL)\n",
    "    df_test = pd.read_csv(test_csv, index_col='ID')\n",
    "\n",
    "    # Impute using combined (train + test) datasets\n",
    "    df_combined = df[feature_cols].append(df_test[feature_cols])\n",
    "    imputer = Imputer(missing_values='NaN', strategy=strategy, axis=0, verbose=0, copy=False).fit(df_combined)\n",
    "    df[feature_cols] = imputer.transform(df[feature_cols])\n",
    "    df_test[feature_cols] = imputer.transform(df_test[feature_cols])\n",
    "    \n",
    "    # Remove duplicate columns and rows\n",
    "    df = munge.remove_duplicates(df.T).T.drop_duplicates()\n",
    "    feature_cols = list(df.columns)\n",
    "    feature_cols.remove(TARGET_COL)\n",
    "    df_test = df_test[feature_cols]\n",
    "    \n",
    "    # Write to file\n",
    "    df.to_csv(train_out_csv, index=False)\n",
    "    feature_cols = list(df.columns)\n",
    "    feature_cols.remove(TARGET_COL)\n",
    "    df_test[feature_cols].to_csv(test_out_csv)\n",
    "    \n",
    "    \n",
    "if not os.path.exists(FILE_TRAIN_DEDUP_ONEHOT_NA_IMPUTE_MEAN):\n",
    "    impute_null_vals(\n",
    "        FILE_TRAIN_DEDUP_ONEHOT_NA,               FILE_TEST_DEDUP_ONEHOT_NA,\n",
    "        FILE_TRAIN_DEDUP_ONEHOT_NA_IMPUTE_MEAN,   FILE_TEST_DEDUP_ONEHOT_NA_IMPUTE_MEAN,\n",
    "        strategy='mean'\n",
    "    )\n",
    "    \n",
    "if not os.path.exists(FILE_TRAIN_DEDUP_ONEHOT_NA_IMPUTE_MEDIAN):\n",
    "    impute_null_vals(\n",
    "        FILE_TRAIN_DEDUP_ONEHOT_NA,               FILE_TEST_DEDUP_ONEHOT_NA,\n",
    "        FILE_TRAIN_DEDUP_ONEHOT_NA_IMPUTE_MEDIAN, FILE_TEST_DEDUP_ONEHOT_NA_IMPUTE_MEDIAN,\n",
    "        strategy='median'\n",
    "    )\n",
    "    \n",
    "if not os.path.exists(FILE_TRAIN_DEDUP_ONEHOT_NA_IMPUTE_FREQ):\n",
    "    impute_null_vals(\n",
    "        FILE_TRAIN_DEDUP_ONEHOT_NA,               FILE_TEST_DEDUP_ONEHOT_NA,\n",
    "        FILE_TRAIN_DEDUP_ONEHOT_NA_IMPUTE_FREQ,   FILE_TEST_DEDUP_ONEHOT_NA_IMPUTE_FREQ,\n",
    "        strategy='most_frequent'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn some of the integer columns to categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_int():\n",
    "    df = pd.read_csv(FILE_TRAIN_DEDUP_ONEHOT_NA)\n",
    "    feature_cols = list(df.columns)\n",
    "    feature_cols.remove(TARGET_COL)\n",
    "    df_test = pd.read_csv(FILE_TEST_DEDUP_ONEHOT_NA, index_col='ID')\n",
    "    \n",
    "    # Ignore already-one hot encoded columns\n",
    "    int_cols = feature_cols[:]\n",
    "    for c in feature_cols:\n",
    "        if c[:6] == 'onehot':\n",
    "            int_cols.remove(c)\n",
    "           \n",
    "    # Fine categorical columns\n",
    "    categorical_cols = eda.find_categorical_columns(df[int_cols], df_test)\n",
    "    \n",
    "    # Convert non-null value containing columns to integers\n",
    "    for c, n in categorical_cols:\n",
    "        # Dont turn null values to int\n",
    "        if c not in {'delta_imp_trasp_var17_in_1y3', 'delta_imp_trasp_var33_in_1y3'}:\n",
    "            df[c] = df[c].values.astype(int)\n",
    "\n",
    "    # One-hot encode the categorical columns\n",
    "    catcols = list(map(lambda t: t[0], categorical_cols))\n",
    "    df_onehot = munge.hash_features(df, columns=catcols)\n",
    "    df_test_onehot = munge.hash_features(df_test, columns=catcols)\n",
    "\n",
    "    # Remove duplicate columns and rows\n",
    "    df_onehot = munge.remove_duplicates(df_onehot.T).T.drop_duplicates()\n",
    "    feature_cols = list(df_onehot.columns)\n",
    "    feature_cols.remove(TARGET_COL)\n",
    "    df_test_onehot = df_test_onehot[feature_cols]\n",
    "    \n",
    "    # Save to file\n",
    "    df_onehot.to_csv(FILE_TRAIN_DEDUP_ONEHOT_NA_ONEHOTINT, index=False)\n",
    "    df_test_onehot.to_csv(FILE_TEST_DEDUP_ONEHOT_NA_ONEHOTINT)\n",
    "\n",
    "    \n",
    "if not os.path.exists(FILE_TRAIN_DEDUP_ONEHOT_NA_ONEHOTINT):\n",
    "    one_hot_int()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find more categorical columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search for equal num unique, but not equal unique vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv(FILE_TRAIN_DEDUP_ONEHOT_NA_ONEHOTINT)\n",
    "# feature_cols = list(df.columns)\n",
    "# feature_cols.remove(TARGET_COL)\n",
    "# df_test = pd.read_csv(FILE_TEST_DEDUP_ONEHOT_NA_ONEHOTINT, index_col='ID')\n",
    "\n",
    "# int_cols = feature_cols[:]\n",
    "# for c in feature_cols:\n",
    "#     if c[:6] == 'onehot':\n",
    "#         int_cols.remove(c)\n",
    "        \n",
    "# same_counts = []\n",
    "# for c in int_cols:\n",
    "#     if df[c].nunique() - 1 == df_test[c].nunique():\n",
    "#         same_counts.append(c)\n",
    "        \n",
    "# x = 20\n",
    "# c = same_counts[x]\n",
    "# print(c)\n",
    "# df[c].value_counts()\n",
    "\n",
    "# df_test[c].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove columns that are constant in test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_test_const_cols():\n",
    "    df = pd.read_csv(FILE_TRAIN_DEDUP_ONEHOT_NA_ONEHOTINT)\n",
    "    feature_cols = list(df.columns)\n",
    "    feature_cols.remove(TARGET_COL)\n",
    "    df_test = pd.read_csv(FILE_TEST_DEDUP_ONEHOT_NA_ONEHOTINT, index_col='ID')\n",
    "    \n",
    "    # Find const cols in test file\n",
    "    test_const_cols = eda.find_const_cols(df_test)\n",
    "    \n",
    "    # Remove const cols\n",
    "    df.drop(test_const_cols, axis=1, inplace=True)\n",
    "    df_test.drop(test_const_cols, axis=1, inplace=True)\n",
    "    \n",
    "    # Remove duplicate rows\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    # Save to file\n",
    "    df.to_csv(FILE_TRAIN_DEDUP_ONEHOT_NA_ONEHOTINT_1TEST, index=False)\n",
    "    df_test.to_csv(FILE_TEST_DEDUP_ONEHOT_NA_ONEHOTINT_1TEST)\n",
    "    \n",
    "if not os.path.exists(FILE_TRAIN_DEDUP_ONEHOT_NA_ONEHOTINT_1TEST):\n",
    "    remove_test_const_cols()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate synthetic records to even out unbalanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(FILE_TRAIN_DEDUP_ONEHOT_NA_ONEHOTINT_1TEST)\n",
    "feature_cols = list(df.columns)\n",
    "feature_cols.remove(TARGET_COL)\n",
    "df_test = pd.read_csv(FILE_TEST_DEDUP_ONEHOT_NA_ONEHOTINT_1TEST, index_col='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    68398\n",
       "1.0     2815\n",
       "Name: TARGET, dtype: int64"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[TARGET_COL].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.297690941385437"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(68398) / 2815"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(FILE_TRAIN_DEDUP_ONEHOT_NA_ONEHOTINT_1TEST)\n",
    "feature_cols = list(df.columns)\n",
    "feature_cols.remove(TARGET_COL)\n",
    "df_test = pd.read_csv(FILE_TEST_DEDUP_ONEHOT_NA_ONEHOTINT_1TEST, index_col='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "saldo_cols = []\n",
    "for c in df:\n",
    "    if c.find('saldo') > -1:\n",
    "        saldo_cols.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_var13_0\n",
      "num_var13_largo_0\n",
      "num_var13_largo\n",
      "num_var13_medio_0\n",
      "num_var13\n",
      "saldo_var13_corto\n",
      "saldo_var13_largo\n",
      "saldo_var13_medio\n",
      "saldo_var13\n",
      "delta_imp_aport_var13_1y3\n",
      "delta_num_aport_var13_1y3\n",
      "imp_aport_var13_hace3\n",
      "imp_aport_var13_ult1\n",
      "imp_reemb_var13_ult1\n",
      "num_aport_var13_hace3\n",
      "num_aport_var13_ult1\n",
      "saldo_medio_var13_corto_hace2\n",
      "saldo_medio_var13_corto_hace3\n",
      "saldo_medio_var13_corto_ult1\n",
      "saldo_medio_var13_corto_ult3\n",
      "saldo_medio_var13_largo_hace2\n",
      "saldo_medio_var13_largo_hace3\n",
      "saldo_medio_var13_largo_ult1\n",
      "saldo_medio_var13_largo_ult3\n",
      "saldo_medio_var13_medio_hace2\n",
      "saldo_medio_var13_medio_ult3\n",
      "onehot_ind_var13_0_0\n",
      "onehot_ind_var13_0_1\n",
      "onehot_ind_var13_corto_0_0\n",
      "onehot_ind_var13_corto_0_1\n",
      "onehot_ind_var13_corto_0\n",
      "onehot_ind_var13_corto_1\n",
      "onehot_ind_var13_largo_0_0\n",
      "onehot_ind_var13_largo_0_1\n",
      "onehot_ind_var13_largo_0\n",
      "onehot_ind_var13_largo_1\n",
      "onehot_ind_var13_medio_0_0\n",
      "onehot_ind_var13_medio_0_1\n",
      "onehot_ind_var13_0\n",
      "onehot_ind_var13_1\n",
      "onehot_num_reemb_var13_ult1_0\n",
      "onehot_num_reemb_var13_ult1_3\n",
      "onehot_num_var13_corto_0_3\n",
      "onehot_num_var13_corto_0_6\n",
      "onehot_num_var13_corto_3\n",
      "onehot_num_var13_corto_6\n",
      "onehot_num_meses_var13_corto_ult3_0\n",
      "onehot_num_meses_var13_corto_ult3_1\n",
      "onehot_num_meses_var13_corto_ult3_2\n",
      "onehot_num_meses_var13_corto_ult3_3\n",
      "onehot_num_meses_var13_largo_ult3_0\n",
      "onehot_num_meses_var13_largo_ult3_1\n",
      "onehot_num_meses_var13_largo_ult3_2\n",
      "onehot_num_meses_var13_largo_ult3_3\n"
     ]
    }
   ],
   "source": [
    "for c in df:\n",
    "    if c.find('var13') > -1:\n",
    "        print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['saldo_var1',\n",
       " 'saldo_var5',\n",
       " 'saldo_var6',\n",
       " 'saldo_var8',\n",
       " 'saldo_var12',\n",
       " 'saldo_var13_corto',\n",
       " 'saldo_var13_largo',\n",
       " 'saldo_var13_medio',\n",
       " 'saldo_var13',\n",
       " 'saldo_var14',\n",
       " 'saldo_var17',\n",
       " 'saldo_var18',\n",
       " 'saldo_var20',\n",
       " 'saldo_var24',\n",
       " 'saldo_var26',\n",
       " 'saldo_var25',\n",
       " 'saldo_var30',\n",
       " 'saldo_var31',\n",
       " 'saldo_var32',\n",
       " 'saldo_var33',\n",
       " 'saldo_var34',\n",
       " 'saldo_var37',\n",
       " 'saldo_var40',\n",
       " 'saldo_var42',\n",
       " 'saldo_var44',\n",
       " 'saldo_medio_var5_hace2',\n",
       " 'saldo_medio_var5_hace3',\n",
       " 'saldo_medio_var5_ult1',\n",
       " 'saldo_medio_var5_ult3',\n",
       " 'saldo_medio_var8_hace2',\n",
       " 'saldo_medio_var8_hace3',\n",
       " 'saldo_medio_var8_ult1',\n",
       " 'saldo_medio_var8_ult3',\n",
       " 'saldo_medio_var12_hace2',\n",
       " 'saldo_medio_var12_hace3',\n",
       " 'saldo_medio_var12_ult1',\n",
       " 'saldo_medio_var12_ult3',\n",
       " 'saldo_medio_var13_corto_hace2',\n",
       " 'saldo_medio_var13_corto_hace3',\n",
       " 'saldo_medio_var13_corto_ult1',\n",
       " 'saldo_medio_var13_corto_ult3',\n",
       " 'saldo_medio_var13_largo_hace2',\n",
       " 'saldo_medio_var13_largo_hace3',\n",
       " 'saldo_medio_var13_largo_ult1',\n",
       " 'saldo_medio_var13_largo_ult3',\n",
       " 'saldo_medio_var13_medio_hace2',\n",
       " 'saldo_medio_var13_medio_ult3',\n",
       " 'saldo_medio_var17_hace2',\n",
       " 'saldo_medio_var17_hace3',\n",
       " 'saldo_medio_var17_ult1',\n",
       " 'saldo_medio_var17_ult3',\n",
       " 'saldo_medio_var29_hace2',\n",
       " 'saldo_medio_var29_ult1',\n",
       " 'saldo_medio_var29_ult3',\n",
       " 'saldo_medio_var33_hace2',\n",
       " 'saldo_medio_var33_hace3',\n",
       " 'saldo_medio_var33_ult1',\n",
       " 'saldo_medio_var33_ult3',\n",
       " 'saldo_medio_var44_hace2',\n",
       " 'saldo_medio_var44_hace3',\n",
       " 'saldo_medio_var44_ult1',\n",
       " 'saldo_medio_var44_ult3']"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saldo_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal(df[['saldo_var13_corto', 'saldo_var13_largo', 'saldo_var13_medio']].sum(axis=1).values, df['saldo_var13'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>saldo_var13_corto</th>\n",
       "      <th>saldo_var13_largo</th>\n",
       "      <th>saldo_var13_medio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>300.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   saldo_var13_corto  saldo_var13_largo  saldo_var13_medio\n",
       "0                0.0                0.0                0.0\n",
       "1              300.0                0.0                0.0\n",
       "2                0.0                0.0                0.0\n",
       "3                0.0                0.0                0.0\n",
       "4                0.0                0.0                0.0"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['saldo_var13_corto', 'saldo_var13_largo', 'saldo_var13_medio']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
